# -*- coding: utf-8 -*-
"""training_on_all.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LXbCq-lih8eyWah5TxkMciaex7RzZ7ZA
"""

!pip install huggingface_hub
!huggingface-cli login


# Transformers installation
! pip install transformers datasets
! pip install transformers datasets evaluate



from datasets import load_dataset

data = load_dataset("nawsh1337/train",split="train")
data2 = load_dataset("nawsh1337/train_to_save_model_1000_rows",split="train")


# data2 = load_dataset("nawsh1337/test",split="test")


print(data2["x"][0],data2["y"][0])

## Preprocess


from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")


def rename_field(example):
    example["text"] = example.pop("x")
    example["label"] = example.pop("y")
    return example

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)


# data = data.map(rename_field,batched=True)

# tokenized_data = data.map(preprocess_function, batched=True)
data2 = data2.map(rename_field,batched=True)
tokenized_data2 = data2.map(preprocess_function, batched=True)

tokenized_data


from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

## Evaluate


import evaluate

accuracy = evaluate.load("accuracy")


import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)


unique_labels = len(data.unique("label"))
unique_labels

values = data.unique("label")
dictionary,inverse_dictionary = {},{}
for i in range(len(values)):
    dictionary[i] = values[i]
for i, value in enumerate(values):
    inverse_dictionary[value] = i
print(dictionary)
print(inverse_dictionary)

values2 = data2.unique("label")
dictionary2,inverse_dictionary2 = {},{}
for i in range(len(values2)):
    dictionary2[i] = values2[i]
for i, value in enumerate(values2):
    inverse_dictionary2[value] = i
print(dictionary2)
print(inverse_dictionary2)


mapping = dictionary
original_list = tokenized_data["label"]
mapped_list = []

for value in original_list:
    for key, val in mapping.items():
        if val == value:
            mapped_list.append(key)

mapping2 = dictionary2
original_list2 = tokenized_data2["label"]
mapped_list2 = []

for value in original_list2:
    for key, val in mapping2.items():
        if val == value:
            mapped_list2.append(key)

import pickle
# with open('mapped_list.pickle', 'wb') as f:
#     pickle.dump(mapped_list, f)
mapped_list = []
with open('mapped_list.pickle', 'rb') as f:
    mapped_list = pickle.load(f)

tokenized_data

def replace_column_values(example, new_values):
    example["label"] = new_values
    return example


# Replace the column values with a new list
# tokenized_data = tokenized_data.map(lambda example: replace_column_values(example, mapped_list))
# tokenized_data = tokenized_data.map(lambda example, i: replace_column_values(example, mapped_list[i]),with_indices=True)
tokenized_data2 = tokenized_data2.map(lambda example, i: replace_column_values(example, mapped_list2[i]),with_indices=True)

tokenized_data2["label"]

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer,EarlyStoppingCallback

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=unique_labels, id2label=dictionary, label2id=inverse_dictionary
)

tokenized_data2[0]

training_args = TrainingArguments(
    output_dir="huggingface.co/nawsh1337/test",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data2,
    eval_dataset=tokenized_data2,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)
# u must push to hub 
trainer.train()



trainer.push_to_hub()



text = "VD BIOTECH"

from transformers import pipeline

classifier = pipeline("sentiment-analysis", model="stevhliu")
classifier(text)


from transformers import AutoTokenizer

tokenizer2 = AutoTokenizer.from_pretrained("nawsh1337/checkpoint-170792")
inputs = tokenizer2(text, return_tensors="pt")

from transformers import AutoModelForSequenceClassification
import torch
model2 = AutoModelForSequenceClassification.from_pretrained("nawsh1337/checkpoint-7500")
with torch.no_grad():
    logits = model2(**inputs).logits

# predicted_class_id = logits.argmax().item()
# model.config.id2label[predicted_class_id]
import torch
count = 0
model2 = model2.to("cpu")
for i,x in enumerate(data["text"]):
  # print(x)
  text = x
  inputs = tokenizer2(text, return_tensors="pt")
  with torch.no_grad():
    logits = model2(**inputs).logits
  predicted_class_id = logits.argmax().item()
  print("Predicted = ",model2.config.id2label[predicted_class_id]," Actual = ",data["label"][i])
  if model2.config.id2label[predicted_class_id] == data["label"][i]:
    count = count + 1
print(count)
